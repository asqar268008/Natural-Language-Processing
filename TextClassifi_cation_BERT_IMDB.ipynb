{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gBl13Uak5WV"
      },
      "source": [
        "Text Classification on BERT\n",
        "Bidirectional Encoder Representations from Transformers (BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-3wYj-Wqlgcc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8WeZQpdmMsm",
        "outputId": "dfa0b310-9c68-4a86-ab7d-b26e7d4a33fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'imdb-dataset-of-50k-movie-reviews' dataset.\n",
            "Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-Xg8BnglqKc",
        "outputId": "8ae68ad7-1250-419c-be1f-63be0b28566c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  review sentiment\n",
            "0      One of the other reviewers has mentioned that ...  positive\n",
            "1      A wonderful little production. <br /><br />The...  positive\n",
            "2      I thought this was a wonderful way to spend ti...  positive\n",
            "3      Basically there's a family where a little boy ...  negative\n",
            "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "...                                                  ...       ...\n",
            "49995  I thought this movie did a down right good job...  positive\n",
            "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
            "49997  I am a Catholic taught in parochial elementary...  negative\n",
            "49998  I'm going to have to disagree with the previou...  negative\n",
            "49999  No one expects the Star Trek movies to be high...  negative\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QEi_k9ZVmXgf"
      },
      "outputs": [],
      "source": [
        "texts = data['review'].tolist()\n",
        "labels = [1 if label == 'positive' else 0 for label in data['sentiment'].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ed3bpUMkmhuL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Split the data into train and text (8:2)\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "#Further splitting the training data into train and validation sets, leaving 60% for training and 20% for testing\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wubQRds_oJMq",
        "outputId": "9f1909ff-fb14-4b90-ec61-88afad9f60f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "#Define the name of the BERT Model\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "\n",
        "max_lenght = 128\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3fRpw3UHo6Ct"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx]\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    #Tokenizer and preprocess the text sample\n",
        "    encoding = self.tokenizer(text,\n",
        "      truncation=True, # Truncate sequence if they exceed the max length.\n",
        "      padding='max_length', # pad sequence to have same length.\n",
        "      max_length=self.max_length, # Truncate or pad text to the specified max length.\n",
        "      return_tensors='pt') #Return the poytorch tensors.\n",
        "\n",
        "    return {\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'label': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTvan0BVq3Dj",
        "outputId": "3faf89ae-cea6-410a-cccf-f82a8e70e117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer arguments: KeysView({'input_ids': tensor([[ 101, 2023, 5691, 2003, 2028, 1997, 1996, 2190, 2537, 1045, 2031, 2464,\n",
            "         1999, 2086, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])})\n",
            "\n",
            " Tokenizer Result size: torch.Size([1, 128])\n",
            "\n",
            " Input ids: tensor([[ 101, 2023, 5691, 2003, 2028, 1997, 1996, 2190, 2537, 1045, 2031, 2464,\n",
            "         1999, 2086, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
            "\n",
            " Token type ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "\n",
            " Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "#Sample Tokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "text = \"This movies is one of the best production I have seen in years.\"\n",
        "\n",
        "encoding = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "\n",
        "print(\"Tokenizer arguments:\", encoding.keys())\n",
        "print(\"\\n Tokenizer Result size:\", encoding['input_ids'].shape)\n",
        "print(\"\\n Input ids:\", encoding['input_ids'])\n",
        "print(\"\\n Token type ids:\", encoding['token_type_ids'])\n",
        "print(\"\\n Attention mask:\", encoding['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PhGf6LPtkfL",
        "outputId": "9432eac3-19a8-4de3-dfce-aa8fe176cbcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: [CLS]\t Input ID: 101\n",
            "Token: this\t Input ID: 2023\n",
            "Token: movies\t Input ID: 5691\n",
            "Token: is\t Input ID: 2003\n",
            "Token: one\t Input ID: 2028\n",
            "Token: of\t Input ID: 1997\n",
            "Token: the\t Input ID: 1996\n",
            "Token: best\t Input ID: 2190\n",
            "Token: production\t Input ID: 2537\n",
            "Token: i\t Input ID: 1045\n",
            "Token: have\t Input ID: 2031\n",
            "Token: seen\t Input ID: 2464\n",
            "Token: in\t Input ID: 1999\n",
            "Token: years\t Input ID: 2086\n",
            "Token: .\t Input ID: 1012\n",
            "Token: [SEP]\t Input ID: 102\n",
            "Token: [PAD]\t Input ID: 0\n"
          ]
        }
      ],
      "source": [
        "input_ids = encoding['input_ids'][0]\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "for token, input_id in zip(tokens, input_ids):\n",
        "  print(f\"Token: {token}\\t Input ID: {input_id}\")\n",
        "  if input_id == 0:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xXr6RAoSuxX4"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_lenght)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_lenght)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o1LLkFsovTDi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K0KKTgBpviMR"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, dropout=0.5):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # hidden_state = outputs.last_hidden_state # This line is not used and can be removed\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = torch.sigmoid(self.fc(x)) # Apply dropout to pooled_output before the linear layer\n",
        "        return logits # No need to return hidden_state if not used later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LsF5shhMxeKy"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, scheduler, devices):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(devices)\n",
        "        attention_mask = batch['attention_mask'].to(devices)\n",
        "        labels = batch['label'].to(devices)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask) # Removed labels argument and _ placeholder\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j9cXCeF3yU46"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def evaluate(model, data_loader, devices):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "          input_ids = batch['input_ids'].to(devices)\n",
        "          attention_mask = batch['attention_mask'].to(devices)\n",
        "          labels = batch['label'].to(devices)\n",
        "\n",
        "\n",
        "          outputs = model(input_ids, attention_mask=attention_mask) # Removed _ placeholder\n",
        "          _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "          predictions.extend(preds.cpu().tolist())\n",
        "          true_labels.extend(labels.cpu().tolist())\n",
        "    return accuracy_score(true_labels, predictions), classification_report(true_labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9Ylo0gFzfHj",
        "outputId": "e00422b9-c613-4629-df0e-cb9bf98130a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "num_classes = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertClassifier(num_classes=num_classes).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9QGLkw4Iz6ky"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "num_epochs = 15\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "train_steps = len(train_loader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=train_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce1d40cb"
      },
      "outputs": [],
      "source": [
        "accuracy_array = []\n",
        "for epoch in range(num_epochs):\n",
        "    train(model, train_loader, optimizer, scheduler, device)\n",
        "    accuracy, report = evaluate(model, val_loader, device)\n",
        "    accuracy_array.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2Pf3MyCn51Pz"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'bert_classifier.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xuznQ4GV7nXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef1e377-2108-4fc2-efed-5eca66ac383d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "load_model = BertClassifier(num_classes).to(device)\n",
        "load_model.load_state_dict(torch.load('bert_classifier.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kpPjGuvB7-NM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25c05ca-a1ce-408a-ce61-b3de21af9ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8889\n",
            "Test Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89      4961\n",
            "           1       0.88      0.90      0.89      5039\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_dataset = TextClassificationDataset(test_texts, test_labels, tokenizer, max_lenght)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "accuracy, report = evaluate(load_model, test_loader, device)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MnY1BVt68zUJ"
      },
      "outputs": [],
      "source": [
        "def predict(model, text, tokenizer, device, max_length = 128):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids = input_ids, attention_mask=attention_mask) # Changed input_id to input_ids\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "    print(f\"Text: {text}, \\n Predicted Sentiment: {predicted.item()}\\n\") # Moved print and added .item()\n",
        "\n",
        "    if predicted.item() == 1: # Changed .items() to .item()\n",
        "        return 'Positive'\n",
        "    else:\n",
        "        return 'Negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RBVkqKW299Bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0067aa1-9bcd-44ba-b5d5-b7623a3c0c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Such a highly-anticipated remake of a cherished musical classic and such a bitter pill it was to have to take. Very, very hard to swallow...all of it. It didn't have an ounce of believability anywhere. And when you don't have a Rose, you don't have a show.<br /><br />Bette Midler seemed born to play this part. Yet, all she was able to produce was a cute, funny, glitzy, trademark Bette Midler...weighed down with all the familiar Midlerisms. Roz Russell has nothing to worry about. She can rest in her grave knowing she is still the definitive Mama Rose (of film, anyway).<br /><br />I thought Midler was really going to put it across this time...to throw herself into what is one of the greatest musical roles of all time...like she did in \"The Rose.\" But, no, she played it safe. She played herself. She made Rose a total dinner-theatre cartoon. Even her songs were uninspired. It was maddening to watch, knowing Midler has the talent to rise above her money-making schtick. She showed promise only once in this \"Gypsy\" and that was with \"Rose's Turn.\" But, by then it was too little, too late.<br /><br />A sincere Cynthia Gibb as the titular heroine gave the film its only true spark and when the role of Gypsy outshines that of Rose, you know there's trouble in River City.<br /><br />A huge, huge letdown., \n",
            " Predicted Sentiment: 0\n",
            "\n",
            "Negative\n",
            "Text: This movie was released originally as a soft \"X\", apparently with the explicit sex deleted. Later, the producers \"relented\" (smelled money) and re-released it with the excised scenes restored (apparently only about 3 minutes). I guess since Kristine was of age, it was held against her and her promising career came grinding to a halt. I guess its all in the timing (witness Pam Anderson's career)--but Ronald Reagan was in charge during Kristine's debacle (we had not heard about Nancy Reagan's affairs), Bill Clinton and Monica Lewinski were in full swing during Pam's \"coming out\".<br /><br />The sex is just icing on the cake, both version satisfy. This naughty musical is way above similar of others that were released at the same time., \n",
            " Predicted Sentiment: 0\n",
            "\n",
            "Negative\n",
            "Text: Having read the books and seen the 1982 Anthony Andrews/Jane<br /><br />Seymour version, I have to say that this is not good at all.<br /><br />According to the books, Percy is supposed to be a seemingly<br /><br />foppish aristocrat when he's being Percy, and witty and clever<br /><br />when he's being the Pimpernel, but here he just looks bored as<br /><br />Percy and mean as the Pimpernel. Marguerite is supposed to be<br /><br />the most beautiful woman in Europe, not a tired and frumpy-looking matron (she looks middle-aged, probably due to<br /><br />bad make-up). Richard E. Grant has done much better things, and<br /><br />Elizabeth McGovern's acting is uninspired and flat. The wit and<br /><br />dash of the books and the Andrews/Seymour film is here replaced<br /><br />by brawn and flashy editing that just don't make the cut. <br /><br /> I might add that to a person who hasn't seen any previous version<br /><br />or read the book, it would probably look ok., \n",
            " Predicted Sentiment: 0\n",
            "\n",
            "Negative\n",
            "Text: Do you like really inventive comedy or do you love \"the wedding crashers\", if the answer is the latter stop reading now. I can't believe this movie is not higher rated. Basically Meadows plays a character not unlike Austin Powers.There are so many inventive moments in this gagorama. From crudity - Leon playing with himself on the porch, the ex boyfriend tricked into eating . . Oh well. To inspired lunacy- clown sex , the Broadway routine, the voice over. Meadows is great as the childish, but very sweet natured Leon. Some great lines \"don't blame the wang\" \"freaky deaky sex world\" too many. . . Why this movie wasn't huge is a mystery. Great comedy., \n",
            " Predicted Sentiment: 1\n",
            "\n",
            "Positive\n",
            "Text: This is a very well-made film, meticulously directed and with some excellent character acting that at times is deeply moving - for example the scene with the loyal but unsophisticated sidekick cop and his wife. The plot is convincingly worked out and exciting. The gangster character is particularly interesting and plays an almost metaphysical role in the life of the hero. It's made clear that the cops are just as rough and ready as the underworld characters.<br /><br />A couple of slight reservations: I found the ending slightly one-sided as it celebrates the hero's successful integration into the structure of the police and justice system, which collapses the ambiguity of the police characters which has been maintained up to that point. Also I found the lead female character somewhat weak: little more than a catalyst for the salvation of the hero, all she seems to do is weep and swoon as the tough guys battle it out., \n",
            " Predicted Sentiment: 1\n",
            "\n",
            "Positive\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "for i in range(5):\n",
        "  text = random.choice(test_texts)\n",
        "  print(predict(load_model, text, tokenizer, device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rVeFmiZ_-ltJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076190dd-4022-4131-fe98-5636b0853059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I am sure this is a masterpiece from the directors, \n",
            " Predicted Sentiment: 1\n",
            "\n",
            "Positive\n"
          ]
        }
      ],
      "source": [
        "text = \"I am sure this is a masterpiece from the directors\"\n",
        "print(predict(load_model, text, tokenizer, device))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}